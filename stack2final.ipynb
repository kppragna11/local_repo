{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import ElasticNet, Lasso, LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, HuberRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=(r'C:\\Users\\Student\\Desktop\\1ga21ec096\\local_repo\\silkboard.csv')\n",
    "df = pd.read_csv(file_path,low_memory=False)\n",
    "\n",
    "# Handle non-numeric values\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Impute missing values with the most frequent value\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "# Display the first few rows of the dataset\n",
    "columns = ['PM10', 'NO', 'NO2', 'NOx', 'NH3', 'SO2', 'CO', 'Ozone', 'Benzene', 'Toluene', 'Temp', 'RH', 'WS', 'WD', 'BP']\n",
    "df_imputed.head()\n",
    "# Define X (features) and y (target variable)\n",
    "X = df_imputed[columns]\n",
    "y = df_imputed['PM2.5']\n",
    " #Define parameters for models\n",
    "params_xgb = {'lambda': 0.7044156083795233, 'alpha': 9.681476940192473, 'colsample_bytree': 0.3, 'subsample': 0.8,\n",
    "              'learning_rate': 0.015, 'max_depth': 3, 'min_child_weight': 235, 'random_state': 48, 'n_estimators': 30000}\n",
    "\n",
    "params_lgb = {'reg_alpha': 4.973064761998367, 'reg_lambda': 0.06365096912006087, 'colsample_bytree': 0.24,\n",
    "              'subsample': 0.8, 'learning_rate': 0.015, 'max_depth': 100, 'num_leaves': 43, 'min_child_samples': 141,\n",
    "              'cat_smooth': 18, 'metric': 'rmse', 'random_state': 48, 'n_estimators': 40000}\n",
    "\n",
    "params_rf= {\n",
    "            'n_estimators': 800,\n",
    "            'max_depth': 5,\n",
    "            'min_samples_split': 3,\n",
    "            'min_samples_leaf': 2}\n",
    "params_gb={\n",
    "       \n",
    "            'n_estimators': 800,\n",
    "            'max_depth': 5,\n",
    "            'learning_rate': 0.01}\n",
    "params_knn={'n_neighbors': 3}\n",
    "params_dt= {\n",
    "       \n",
    "            'max_depth': 5, \n",
    "            'min_samples_split': 5,\n",
    "            'min_samples_leaf': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = np.zeros(df_imputed.shape[0])\n",
    "pred2 = np.zeros(df_imputed.shape[0])\n",
    "pred3 = np.zeros(df_imputed.shape[0])\n",
    "pred4 = np.zeros(df_imputed.shape[0])\n",
    "pred5 = np.zeros(df_imputed.shape[0])\n",
    "pred6 = np.zeros(df_imputed.shape[0])\n",
    "pred7 = np.zeros(df_imputed.shape[0])\n",
    "pred9 = np.zeros(df_imputed.shape[0])\n",
    "pred10 = np.zeros(df_imputed.shape[0])\n",
    "pred11 = np.zeros(df_imputed.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000349 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3363\n",
      "[LightGBM] [Info] Number of data points in the train set: 52326, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score 23.307948\n"
     ]
    }
   ],
   "source": [
    "# Use LeaveOneOut for outer cross-validation\n",
    "loo = LeaveOneOut()\n",
    "n = 0\n",
    "for trn_idx, test_idx in loo.split(X, y):\n",
    "    print(f\"fold: {n+1}\")\n",
    "    X_tr, X_val = X.iloc[trn_idx], X.iloc[test_idx]\n",
    "    y_tr, y_val = y.iloc[trn_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Model 1: LGBMRegressor\n",
    "    model1 = lgb.LGBMRegressor(**params_lgb)\n",
    "    model1.fit(X_tr, y_tr)\n",
    "    pred1[test_idx] = model1.predict(X_val)\n",
    "\n",
    "    # Model 2: ElasticNet\n",
    "    model2 = ElasticNet(alpha=0.00001, max_iter=10000)\n",
    "    model2.fit(X_tr, y_tr)\n",
    "    pred2[test_idx] = model2.predict(X_val)\n",
    "\n",
    "    # Model 3: LinearRegression\n",
    "    model3 = LinearRegression()\n",
    "    model3.fit(X_tr, y_tr)\n",
    "    pred3[test_idx] = model3.predict(X_val)\n",
    "\n",
    "    # Model 4: XGBRegressor\n",
    "    model4 = xgb.XGBRegressor(**params_xgb)\n",
    "    model4.set_params(early_stopping_rounds=200)\n",
    "    model4.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    pred4[test_idx] = model4.predict(X_val)\n",
    "\n",
    "    # Model 5: Ridge\n",
    "    model5 = Ridge(alpha=1.0)\n",
    "    model5.fit(X_tr, y_tr)\n",
    "    pred5[test_idx] = model5.predict(X_val)\n",
    "\n",
    "    # Model 6: RandomForestRegressor\n",
    "    model6 = RandomForestRegressor(**params_rf)\n",
    "    model6.fit(X_tr, y_tr)\n",
    "    pred6[test_idx] = model6.predict(X_val)\n",
    "\n",
    "    # Model 7: HuberRegressor\n",
    "    model7 = HuberRegressor(epsilon=1.2, max_iter=1000)\n",
    "    scaler = StandardScaler()\n",
    "    model7 = make_pipeline(scaler, model7)\n",
    "    model7.fit(X_tr, y_tr)\n",
    "    pred7[test_idx] = model7.predict(X_val)\n",
    "\n",
    "    # Model 9: GradientBoostingRegressor\n",
    "    model9 = GradientBoostingRegressor(n_estimators=800, max_depth=5, learning_rate=0.01, random_state=42)\n",
    "    model9.fit(X_tr, y_tr)\n",
    "    pred9[test_idx] = model9.predict(X_val)\n",
    "\n",
    "    # Model 10: KNeighborsRegressor\n",
    "    model10 = KNeighborsRegressor(**params_knn)\n",
    "    model10.fit(X_tr, y_tr)\n",
    "    pred10[test_idx] = model10.predict(X_val)\n",
    "\n",
    "    # Model 11: DecisionTreeRegressor\n",
    "    model11 = DecisionTreeRegressor(**params_dt)\n",
    "    model11.fit(X_tr, y_tr)\n",
    "    pred11[test_idx] = model11.predict(X_val)\n",
    "\n",
    "    n += 1\n",
    "\n",
    "# Stack the predictions\n",
    "stacked_predictions = np.column_stack((pred1, pred2, pred3, pred4, pred5, pred6, pred7, pred9, pred10, pred11))\n",
    "\n",
    "# Define the meta-model (GradientBoostingRegressor)\n",
    "meta_model = GradientBoostingRegressor(n_estimators=800, max_depth=5, learning_rate=0.01, random_state=42)\n",
    "\n",
    "# Train the meta-model on stacked predictions\n",
    "meta_model.fit(stacked_predictions, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = df_imputed[columns] \n",
    "# Make predictions using base models\n",
    "pred1_test = model1.predict(test_data)\n",
    "pred2_test = model2.predict(test_data)\n",
    "pred3_test = model3.predict(test_data)\n",
    "pred4_test = model4.predict(test_data)\n",
    "pred5_test = model5.predict(test_data)\n",
    "pred6_test = model6.predict(test_data)\n",
    "pred7_test = model7.predict(test_data)\n",
    "\n",
    "pred9_test = model9.predict(test_data)\n",
    "pred10_test = model10.predict(test_data)\n",
    "pred11_test = model11.predict(test_data)\n",
    "# Stack the predictions\n",
    "stacked_predictions_test = np.column_stack((pred1_test, pred2_test, pred3_test, pred4_test,pred5_test,pred6_test,pred7_test,pred9_test,pred10_test,pred11_test))\n",
    "\n",
    "# Use the meta-model to make final predictions\n",
    "final_predictions_test = meta_model.predict(stacked_predictions_test)\n",
    "y_test=df_imputed['PM2.5']\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "final_rmse = mean_squared_error(y_test, final_predictions_test, squared=False)\n",
    "mse_stacked = mean_squared_error(y_test, final_predictions_test)\n",
    "rmse_stacked = np.sqrt(mse_stacked)\n",
    "r2_stacked = r2_score(y_test, final_predictions_test)\n",
    "print(f\"Final RMSE on the test set: {final_rmse}\")\n",
    "print(f\"Final RMSE_STACK on the test set: {rmse_stacked}\")\n",
    "print(f\"Final MSE on the test set: {mse_stacked}\")\n",
    "print(f\"Final r2 on the test set: {r2_stacked}\")\n",
    "\n",
    "plt.scatter(y_test,final_predictions_test)\n",
    "plt.xlabel('True Values ')\n",
    "plt.ylabel('Predictions ')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
